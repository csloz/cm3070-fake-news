{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection \n",
    "## CM3070 Prototype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Environment Setup (setup a venv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install virtualenv ipykernel\n",
    "# virtualenv cm3070\n",
    "# source cm3070/bin/activate\n",
    "# python -m ipykernel install --user --name=cm3070\n",
    "\n",
    "#Installed kernelspec cm3070 in /Users/lawrence/Library/Jupyter/kernels/cm3070\n",
    "#Restart VS Code to see new kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Library Setup (install required libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML, NLK and other classification libraries\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install transformers\n",
    "%pip install ekphrasis\n",
    "%pip install keras-tuner\n",
    "%pip install flair\n",
    "%pip install nltk\n",
    "%pip install tensorflow\n",
    "\n",
    "#For Jupyter Progress bar bits\n",
    "%pip install iprogress\n",
    "%pip install ipywidgets\n",
    "\n",
    "#output libs\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip datasets if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Path is: \"/Users/lawrence/Documents/CourseWork/Level 6/CM3070 - Final/code\"\n",
      "Extracted all_test_public\n",
      "Extracted all_train\n",
      "Extracted all_validate\n"
     ]
    }
   ],
   "source": [
    "### Unzip datasets if necessary\n",
    "#Only FAKEDDIT are compressed initially, if from git. \n",
    "#This will iterate and unzip if needed on initial run.\n",
    "\n",
    "import zipfile\n",
    "import os.path, os\n",
    "\n",
    "print (f'Working Path is: \"{os.getcwd()}\"')\n",
    "\n",
    "prefix = \"./datasets/FAKEDDIT/\"\n",
    "\n",
    "fileset = ['all_test_public','all_train','all_validate']\n",
    "\n",
    "for file in fileset:\n",
    "    if not os.path.isfile(f'{prefix}{file}.tsv'):\n",
    "        with zipfile.ZipFile(f'{prefix}{file}.zip',\"r\") as zip_ref:\n",
    "            zip_ref.extractall(prefix)\n",
    "            print (f'Extracted {file}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import flair\n",
    "import urllib\n",
    "import statistics\n",
    "import math\n",
    "import pprint\n",
    "import sklearn\n",
    "\n",
    "#Import the NLP cleaning pre-processing tools\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet') #currently used\n",
    "nltk.download('stopwords') #currently used\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "\n",
    "#Import Notebook bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#Get our Evaluation metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Output\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add pretty status progress bar status to notebook\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "### 1.1 - Import and Normalize Benchmark Data\n",
    "* Import to named df and concatencate data as needed.\n",
    "* Add is_true column.\n",
    "* Flatten truthiness to true/false class.\n",
    "* Set appropriate description column from dataset, and move column to before is_true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_isot():\n",
    "    #Import both csv's\n",
    "    isot_true = pd.read_csv('./datasets/ISOT/True.csv')\n",
    "    isot_fake = pd.read_csv('./datasets/ISOT/Fake.csv')\n",
    "    #Add truthiness column\n",
    "    isot_true['is_fake']=False\n",
    "    isot_fake['is_fake']=True\n",
    "    #Concat both CSV's\n",
    "    df = pd.concat([isot_true, isot_fake])\n",
    "\n",
    "    #rename 2 -> description\n",
    "    df.rename(columns={ \"text\": \"description\" }, inplace = True)\n",
    "    #move to end -1\n",
    "    df.insert(len(df.columns)-2, 'description', df.pop('description')) #-2 as starts from 0\n",
    "    #need to flatten out half truth etc -> true / false\n",
    "    \n",
    "    return df\n",
    "\n",
    "def import_liar():\n",
    "    df = pd.read_csv('./datasets/LIAR/test.tsv',  sep='\\t',  header=None)\n",
    "    #add our truthiness column, and capitalize it to conform to True False / Other\n",
    "    df['is_fake'] = df.iloc[:, 1].str.capitalize()\n",
    "    \n",
    "    #rename 2 -> description\n",
    "    df.rename(columns={ df.columns[2]: \"description\" }, inplace = True)\n",
    "    #move to end -1\n",
    "    df.insert(len(df.columns)-2, 'description', df.pop('description')) #-2 as starts from 0\n",
    "    #need to flatten out half truth etc -> true / false\n",
    "    #tbd\n",
    "    return df\n",
    "\n",
    "def import_fnn():\n",
    "    #Import\n",
    "    fnn_gc_fake = pd.read_csv('./datasets/FakeNewsNet/dataset/gossipcop_fake.csv')\n",
    "    fnn_gc_true = pd.read_csv('./datasets/FakeNewsNet/dataset/gossipcop_real.csv')\n",
    "    fnn_p_fake = pd.read_csv('./datasets/FakeNewsNet/dataset/politifact_fake.csv')\n",
    "    fnn_p_true = pd.read_csv('./datasets/FakeNewsNet/dataset/politifact_real.csv')\n",
    "    #Add truthiness colum\n",
    "    fnn_gc_true['is_fake']=False\n",
    "    fnn_gc_fake['is_fake']=True\n",
    "    fnn_p_true['is_fake']=False\n",
    "    fnn_p_fake['is_fake']=True\n",
    "    #make into a single set\n",
    "    df = pd.concat ([fnn_gc_fake, fnn_gc_true, fnn_p_fake, fnn_p_true])\n",
    "    df.rename(columns={\"title\": \"description\"},inplace=True)\n",
    "    #Move to end -1\n",
    "    df.insert(len(df.columns)-2, 'description', df.pop('description')) #-2 as starts from 0\n",
    "    return df\n",
    "\n",
    "def import_fe():\n",
    "    df = pd.read_csv('./datasets/FAKEDDIT/all_test_public.tsv', sep='\\t')\n",
    "    df.rename(columns={\"title\": \"description\"},inplace=True)\n",
    "    df['is_fake'] = df['2_way_label']\n",
    "    #Move to end -1\n",
    "    df.insert(len(df.columns)-2, 'description', df.pop('description')) #-2 as starts from 0\n",
    "    #Need to amend 2_way_label -> true / false\n",
    "    df.is_fake = df.is_fake.replace({ 0: True,  1:False})\n",
    "    #dropna\n",
    "    df = df[df['description'].notna()]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def df_stats(name, df, showdf=False):\n",
    "    #Display basic stats on data size\n",
    "    print (f'{name} Size{df.shape}')\n",
    "    \n",
    "    print (df.is_fake.value_counts())\n",
    "    if (showdf):\n",
    "        display(df)\n",
    "    #Display Pie chart of Fake/True counts\n",
    "    pie_df =  df['is_fake'].value_counts()\n",
    "    #total = df['is_fake'].value_counts().values.sum()\n",
    "    plot = pie_df.plot.pie(figsize=(5, 5),legend=False,  autopct='%1.1f%%' )\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 - LIAR Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Import and Mangling\n",
    "\n",
    "#LIAR\n",
    "liar_df = import_liar()\n",
    "df_stats(\"LIAR\", liar_df) #balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 - ISOT Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ISOT\n",
    "isot_df = import_isot()\n",
    "df_stats(\"ISOT\", isot_df) #balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 - FakeNewsNet Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FakeNewsNet\n",
    "fnn_df = import_fnn()\n",
    "df_stats(\"FakeNewsNet\", fnn_df) #unbalanced data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 - Fakeddit Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#FAKEDDIT\n",
    "fe_df = import_fe()\n",
    "df_stats(\"Fakeddit\", fe_df, False) #balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "\n",
    "def clean_text(s):\n",
    "    #Lowercase, remove html, strip non-alphanumeric, remove spaces, remove stop words, lemmatize\n",
    "\n",
    "    #if s is NaN then exit\n",
    "    # if not s==s: \n",
    "    #     return s\n",
    "    \n",
    "    #1 - lowercase\n",
    "    s = s.lower() \n",
    "\n",
    "    #2 - remove html using basic regex.  \n",
    "    dehtml = re.compile(r'<[^>]*>')\n",
    "    s= dehtml.sub('', s)\n",
    "\n",
    "    #3 - Strip non alphanumeric away to spaces\n",
    "    s = re.sub (r'[^a-z0-9\\s]',' ',s) \n",
    "    \n",
    "    #4 - Remove excess spaces\n",
    "    s = re.sub (r'\\s+',' ', s).strip() \n",
    "\n",
    "    #5 - remove stop words from the sentence \n",
    "    tokens=  s.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [token for token in tokens if token not in stop_words]\n",
    "    s = \" \".join(filtered)\n",
    "\n",
    "    #6 - lemmatize and remove from sentence\n",
    "    tokens = s.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    s = \" \".join (filtered)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Clean a passed in dataframe desc column -> clean_text\n",
    "def clean_df (df):\n",
    "    #copy the df\n",
    "    clean = df.copy()\n",
    "    #add our processed column with tqdm goodness for progress\n",
    "    clean['clean_description'] = clean['description'].progress_apply (clean_text)\n",
    "    #reset the df\n",
    "    clean.reset_index(inplace=True, drop=True)\n",
    "    #Move clean_description to before is_true for ease of visibility\n",
    "    clean.insert(len(df.columns)-1, 'clean_description', clean.pop('clean_description'))\n",
    "    display(clean.head())\n",
    "    return clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (clean_text('This IS a Lawrence\\'s of Arabia\\'s <B>rather</B> #Brilliant \\t  <i>set</i> of  lots of SentEnces...'))\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_df = clean_df(liar_df)\n",
    "\n",
    "isot_df = clean_df(isot_df)\n",
    "\n",
    "fnn_df = clean_df(fnn_df)\n",
    "\n",
    "fe_df = clean_df(fe_df)\n",
    "\n",
    "print (\"Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collated = [liar_df[['description', 'clean_description', 'is_fake']], \n",
    "            isot_df[['description', 'clean_description', 'is_fake']],\n",
    "            fnn_df[['description', 'clean_description', 'is_fake']],\n",
    "            fe_df[['description', 'clean_description', 'is_fake']],\n",
    "            ]\n",
    "combined_df = pd.concat(collated)\n",
    "\n",
    "df_stats (\"Combined\",combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Feature Extraction\n",
    "\n",
    "#### Testing Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "sentence = (clean_text('This IS a Lawrence\\'s of Arabia\\'s <B>rather</B> #Brilliant \\t  <i>set</i> of  lots of SentEnces...'))\n",
    "print (sentence)\n",
    "# create a sentence\n",
    "sentence = Sentence(sentence)\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# predict the named entities in the sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the predicted named entities\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "\n",
    "#can see named entity is listed for lawrence (of) arabia -> person "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prototype Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prototype uses a single test classifier and feature extractor  \n",
    "classifiers=dict()\n",
    "classifiers['dt'] = DecisionTreeClassifier()\n",
    "\n",
    "extractors=dict()\n",
    "extractors['tfidf'] = TfidfVectorizer(stop_words='english', max_df=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate (y_true, y_predicted):\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_predicted) \n",
    "    sns.heatmap(cm, annot=True, cmap='rocket_r', fmt= '.5g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.show()\n",
    "    # F1 Scoring and Accuracy\n",
    "    print('') \n",
    "    display('Accuracy, Precision, Recall and F1-score')\n",
    "    print(classification_report(y_true, y_predicted, digits=5))\n",
    "\n",
    "\n",
    "def train (name, x_train, x_test, y_train, y_test):\n",
    "    #name datasource name\n",
    "    #x_train, x_test use the extractors to train\n",
    "    #y_train, y_test use the classifiers to train\n",
    "\n",
    "    transformed_train=extractors['tfidf'].fit_transform(x_train)\n",
    "    transformed_test=extractors['tfidf'].transform(x_test)\n",
    "\n",
    "    classifiers['dt'].fit (transformed_train, y_train)\n",
    "    y_pred = classifiers['dt'].predict(transformed_test)\n",
    "\n",
    "    print (name)\n",
    "    evaluate (y_test,y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Classification (Prediction Scoring)\n",
    "#### Single example shown for video for speed reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#isot_df\n",
    "#liar_df\n",
    "#fnn_df\n",
    "#fe_df\n",
    "#combined_df\n",
    "\n",
    "training = dict()\n",
    "\n",
    "training['ISOT'] = isot_df #too slow to show more than one training sample in video\n",
    "#training['LIAR'] = liar_df\n",
    "#training['FNN'] = fnn_df\n",
    "#training['FEDDIT'] = fe_df\n",
    "#training['COMBINED']= combined_df\n",
    " \n",
    "\n",
    "for name, df in tqdm(training.items()):\n",
    "    labels = df.is_fake\n",
    "    x_train, x_test, y_train, y_test=train_test_split(isot_df['clean_description'], labels, test_size=0.3, random_state=7)\n",
    "\n",
    "    train ( name,x_train, x_test, y_train, y_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added at a later stage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm3070",
   "language": "python",
   "name": "cm3070"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
